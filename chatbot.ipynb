{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doğukan', 've', 'veya', 'ben', 'çalışıyoruz']\n",
      "['Doğukan', 'ben', 'çalışıyoruz'] \n",
      "\n",
      "{'nasıl', 'hep', 'hem', 'çünkü', 'mı', 'şu', 'kim', 'daha', 'ama', 've', 'siz', 'hiç', 'eğer', 'nerede', 'en', 'nerde', 'birşey', 'için', 'niye', 'tüm', 'biri', 'acaba', 'her', 'da', 'sanki', 'aslında', 'mü', 'diye', 'niçin', 'az', 'neden', 'hepsi', 'ya', 'de', 'şey', 'veya', 'mu', 'gibi', 'nereye', 'ki', 'bu', 'çok', 'ise', 'yani', 'ile', 'bazı', 'ne', 'kez', 'birkaç', 'defa', 'biz', 'o', 'belki'}\n"
     ]
    }
   ],
   "source": [
    "# from TurkishStemmer import TurkishStemmer\n",
    "# stemmer = TurkishStemmer()\n",
    "\n",
    "# from snowballstemmer import TurkishStemmer\n",
    "# turkStem = TurkishStemmer()\n",
    "# # a = (\"bilimsel\")\n",
    "\n",
    "# # a = [(stemmer.stem(nltk.word_tokenize(a.lower())))]\n",
    "# # print(a)\n",
    "# print(turkStem.stemWord(\"bilimsel\"))\n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "  \n",
    "example_sent = \"\"\"Doğukan ve veya ben çalışıyoruz\"\"\"\n",
    "  \n",
    "stop_words = set(stopwords.words('turkish'))  \n",
    "  \n",
    "word_tokens = word_tokenize(example_sent)  \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "  \n",
    "# filtered_sentence = []  \n",
    "  \n",
    "# for w in word_tokens:  \n",
    "#     if w not in stop_words:  \n",
    "#         filtered_sentence.append(w)  \n",
    "  \n",
    "print(word_tokens)  \n",
    "print(filtered_sentence,\"\\n\")  \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 13999  | total loss: \u001b[1m\u001b[32m0.43258\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 1000 | loss: 0.43258 - acc: 0.9132 -- iter: 1170/1208\n",
      "Training Step: 14000  | total loss: \u001b[1m\u001b[32m0.41561\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 1000 | loss: 0.41561 - acc: 0.9107 -- iter: 1208/1208\n",
      "--\n",
      "INFO:tensorflow:/home/yazilimci/Documents/chatbot/5ocak/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "# nltk.download('punkt')\n",
    "import time \n",
    "\n",
    "\n",
    "import numpy \n",
    "import tflearn \n",
    "import tensorflow \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import random \n",
    "import json \n",
    "import pickle\n",
    "from snowballstemmer import TurkishStemmer\n",
    "turkStem = TurkishStemmer()\n",
    "\n",
    "# with open('my_intents2.json') as file:\n",
    "#     data = json.load(file)\n",
    "#     print(\"merhabaaaaaaaaa\")\n",
    "\n",
    "with open('intents_.json') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "\n",
    "try: \n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except: \n",
    "    \n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = [] #2.part\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            \n",
    "            words.extend(wrds) #iki diziyi birleştirir.\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "            \n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])     \n",
    "            \n",
    "    words = [turkStem.stemWord(w.lower()) for w in words if w != \"?\"] # tüm pattern'ın içerisindeki kelimeleri parçalayıp attık.\n",
    "    words = sorted(list(set(words)))\n",
    "    \n",
    "    labels = sorted(labels)\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "\n",
    "# until 2-part\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [turkStem.stemWord(w.lower()) for w in doc]\n",
    "    \n",
    "    for w in words:\n",
    "       \n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "       \n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "    \n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "    \n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "# with open(\"data.pickle\",\"wb\") as f:\n",
    "#     pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "# 3-part\n",
    "ops.reset_default_graph()\n",
    "# tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net,8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "\n",
    "# model.load(\"model.tflearn\")\n",
    "#batch_size : girdilerin sayısı\n",
    "model.fit(training, output, n_epoch=1000, batch_size=90, show_metric=True)\n",
    "model.save(\"model.tflearn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [turkStem.stemWord(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "import pyttsx3\n",
    "\n",
    "def chat():\n",
    "    print(\"Merhaba benim adım Gazi:) \\nÜniversitemiz hakkında merak ettiğin bir konu varsa bana sorabilirsin.\")\n",
    "    while True:\n",
    "        inp = input(\"Sen: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        inp = inp.split(' ')\n",
    "        print(\"Önce -->>>\",inp)\n",
    "        string = \"\"\n",
    "        empt=\" \"\n",
    "        for indx, i in enumerate(inp):\n",
    "            inp[indx] = my_autocorrect(i)\n",
    "            \n",
    "        print(\"SOnra--->>>\",inp)\n",
    "        inp_ = empt.join(inp)\n",
    "        print(inp_)\n",
    "        results = model.predict([bag_of_words(inp_, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        \n",
    "        accurate = results[0][results_index]\n",
    "        \n",
    "        print('********************\\n')\n",
    "        tag = labels[results_index]\n",
    "#         print(\"TAGG ==== \",tag)\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "                \n",
    "        if accurate < 0.5:\n",
    "            print(\"Bu konuyu mu aramıştınız: \", tag, \"    accurate=>\", accurate )\n",
    "            inp2 = input(\"evetse 1 e hayır ise 2'ye basınız : \")\n",
    "            if inp2 == \"1\":\n",
    "                print(responses)\n",
    "            else :\n",
    "                print(\"Doğru sözcükler ile tekrar deneyin\")\n",
    "\n",
    "        else: \n",
    "            print(responses)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_autocorrect('mevla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textdistance\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "def my_autocorrect(input_word):\n",
    "    with open('intents_.json') as f:\n",
    "      data = json.load(f)\n",
    "    data = data['intents']\n",
    "    # data.loc['patterns']\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    # data = data[0]\n",
    "    list_ = []\n",
    "    for i in data: \n",
    "        for t in i['patterns']:\n",
    "            a = word_tokenize(t.lower()) \n",
    "            for j in a :\n",
    "                list_.append(j)\n",
    "    # data\n",
    "#     print(len(list_))\n",
    "    V = set(list_)\n",
    "#     print(len(V))\n",
    "\n",
    "    word_freq_dict = {}  \n",
    "    word_freq_dict = Counter(list_)\n",
    "#     print(word_freq_dict.most_common()[0:10])\n",
    "\n",
    "    probs = {} \n",
    "\n",
    "    Total = sum(word_freq_dict.values())\n",
    "\n",
    "    for k in word_freq_dict.keys():\n",
    "        probs[k] = word_freq_dict[k]/Total\n",
    "\n",
    "\n",
    "    input_word = input_word.lower()\n",
    "    if input_word in V:\n",
    "        return input_word\n",
    "    else:\n",
    "        similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()]\n",
    "        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
    "        df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
    "        df['Similarity'] = similarities\n",
    "        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\n",
    "        return(output['Word'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
